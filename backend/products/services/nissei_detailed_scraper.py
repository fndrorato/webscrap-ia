import base64
import os
import re
import requests
import time
import urllib.request
import uuid
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from decimal import Decimal, InvalidOperation
from django.utils import timezone
from django.core.files.base import ContentFile
from PIL import Image
from io import BytesIO
from products.models import Product, ProductImage
from sites.models import Site


# CONFIGURA√á√ÉO ADICIONAL para PIL
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True  # Permite imagens truncadas

class NisseiDetailedScraper:
    """
    Scraper completo do Nissei que visita cada produto individualmente
    para extrair descri√ß√£o detalhada e baixar todas as imagens
    """
    
    def __init__(self, site: Site):
        self.site = site
        self.base_url = "https://nissei.com"
        self.currency = "Gs."
        
        # Configurar sess√£o HTTP
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-419,es;q=0.9,en;q=0.8,pt;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
        })
        
        # Configura√ß√µes
        self.delay_between_requests = 3  # Mais tempo entre requests
        self.delay_between_products = 5  # Pausa entre produtos individuais
        self.max_retries = 3
        self.max_images_per_product = 3
        
        # Valida√ß√£o de imagens
        self.max_image_size = 10 * 1024 * 1024  # 10MB
        self.min_image_size = 1024  # 1KB
        self.supported_formats = ['jpeg', 'jpg', 'png', 'webp']
    
    def scrape_products_detailed(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """
        Scraping completo com extra√ß√£o detalhada de cada produto
        """
        try:
            print(f"üîç SCRAPING DETALHADO - Busca por '{query}' no Nissei.com")
            print("=" * 60)
            
            # FASE 1: Obter lista b√°sica de produtos
            print("üìã FASE 1: Obtendo lista de produtos...")
            basic_products = self._get_basic_product_list(query, max_results)
            
            if not basic_products:
                print("‚ùå Nenhum produto encontrado na listagem")
                return []
            
            print(f"‚úÖ {len(basic_products)} produtos encontrados na listagem")
            print("=" * 60)
            
            # FASE 2: Processar cada produto individualmente
            print("üîé FASE 2: Processando produtos individualmente...")
            detailed_products = []
            
            for i, basic_product in enumerate(basic_products, 1):
                try:
                    print(f"\nüì± PRODUTO {i}/{len(basic_products)}: {basic_product['name'][:50]}...")
                    print(f"üåê URL: {basic_product['url']}")
                    
                    # Extrair detalhes do produto individual
                    detailed_product = self._extract_product_details(basic_product)
                    
                    if detailed_product:
                        detailed_products.append(detailed_product)
                        print(f"‚úÖ Produto processado com sucesso")
                        
                        # Baixar imagens do produto
                        print("üì∏ Baixando imagens...")
                        image_count = self._download_product_images(detailed_product)
                        print(f"üì∏ {image_count} imagens baixadas")
                    else:
                        print("‚ùå Falha ao processar produto")
                    
                    # Rate limiting entre produtos
                    if i < len(basic_products):
                        print(f"‚è±Ô∏è Aguardando {self.delay_between_products}s...")
                        time.sleep(self.delay_between_products)
                        
                except Exception as e:
                    print(f"‚ùå Erro ao processar produto {i}: {str(e)}")
                    continue
            
            # FASE 3: Salvar no banco de dados
            print(f"\nüíæ FASE 3: Salvando {len(detailed_products)} produtos...")
            saved_count = self._save_detailed_products(detailed_products)
            
            print(f"üéâ SCRAPING CONCLU√çDO!")
            print(f"üìä Produtos processados: {len(detailed_products)}")
            print(f"üíæ Produtos salvos: {saved_count}")
            
            return detailed_products
            
        except Exception as e:
            print(f"‚ùå Erro geral no scraping detalhado: {str(e)}")
            import traceback
            print(traceback.format_exc())
            return []
    
    def _get_basic_product_list(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Fase 1: Obter lista b√°sica de produtos da busca"""
        try:
            query_encoded = query.replace(' ', '+')
            search_url = f"{self.base_url}/py/catalogsearch/result/?q={query_encoded}"
            
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Usar os mesmos seletores que funcionaram
            product_elements = soup.select('.product-item')[:max_results]
            
            basic_products = []
            for element in product_elements:
                try:
                    # Extrair apenas o essencial da listagem
                    name_elem = element.select_one('.product-item-name a, .product-name a, h3 a')
                    name = name_elem.get_text(strip=True) if name_elem else 'Sem nome'
                    
                    link_elem = element.find('a', href=True)
                    product_url = ''
                    if link_elem:
                        href = link_elem.get('href', '')
                        if href.startswith('/'):
                            product_url = f"{self.base_url}{href}"
                        elif href.startswith('http'):
                            product_url = href
                    
                    if name and product_url and len(name) > 3:
                        basic_products.append({
                            'name': name,
                            'url': product_url,
                            'search_query': query
                        })
                        
                except Exception as e:
                    continue
            
            return basic_products
            
        except Exception as e:
            print(f"‚ùå Erro ao obter lista b√°sica: {str(e)}")
            return []

    def _extract_product_categories(self, soup: BeautifulSoup) -> List[str]:
        """Extrai categorias do produto - NOVO M√âTODO"""
        categories = []
        
        print(f"üè∑Ô∏è Procurando categorias...")
        
        # Seletores para breadcrumbs/navega√ß√£o
        breadcrumb_selectors = [
            '.breadcrumbs li a',
            '.breadcrumb-item a',
            '.breadcrumb a',
            '.navigation .crumb a',
            '.page-header .breadcrumbs a',
            '.toolbar-breadcrumbs a'
        ]
        
        for selector in breadcrumb_selectors:
            try:
                links = soup.select(selector)
                print(f"  üéØ Breadcrumb '{selector}': {len(links)} items")
                
                for link in links:
                    category_text = link.get_text(strip=True)
                    if category_text and len(category_text) > 2:
                        # Filtrar categorias irrelevantes
                        if category_text.lower() not in ['home', 'inicio', 'principal']:
                            categories.append(category_text)
                            print(f"    üìÇ Categoria: {category_text}")
                
                if categories:
                    break
                    
            except Exception as e:
                continue
        
        # Se n√£o encontrou via breadcrumbs, tentar outras abordagens
        if not categories:
            print(f"‚ö†Ô∏è Tentando encontrar categorias por outros m√©todos...")
            
            # Procurar em meta tags
            meta_category = soup.find('meta', {'name': 'category'})
            if meta_category:
                categories.append(meta_category.get('content', ''))
            
            # Procurar em dados estruturados
            structured_data = soup.find('script', {'type': 'application/ld+json'})
            if structured_data:
                try:
                    data = json.loads(structured_data.string)
                    if 'category' in data:
                        categories.append(data['category'])
                except:
                    pass
        
        print(f"üìä Total de categorias encontradas: {len(categories)}")
        return list(set(categories))

    def _extract_specifications_improved(self, soup: BeautifulSoup) -> Dict[str, str]:
        """Extrai especifica√ß√µes t√©cnicas - VERS√ÉO MELHORADA"""
        specs = {}
        
        print(f"üîß Procurando especifica√ß√µes t√©cnicas...")
        
        # Seletores para tabelas de especifica√ß√µes
        spec_table_selectors = [
            '#additional .data.table tbody tr',
            '.additional-attributes-wrapper .data.table tr',
            '.product-specifications tbody tr',
            '.product-attributes tr',
            '.data.table.additional-attributes tr',
            '.spec-table tr',
            '.features-table tr'
        ]
        
        for selector in spec_table_selectors:
            try:
                rows = soup.select(selector)
                print(f"  üéØ Especifica√ß√µes '{selector}': {len(rows)} linhas")
                
                for row in rows:
                    cells = row.select('td, th')
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        if key and value and len(key) < 100:  # Filtrar keys muito longas
                            specs[key] = value
                            print(f"    üìã {key}: {value}")
                
                if specs:
                    break
                    
            except Exception as e:
                continue
        
        # Se n√£o encontrou em tabelas, procurar em listas
        if not specs:
            list_selectors = [
                '.product-specs li',
                '.specifications li',
                '.features li',
                '.product-features li'
            ]
            
            for selector in list_selectors:
                try:
                    items = soup.select(selector)
                    for item in items:
                        text = item.get_text(strip=True)
                        if ':' in text:
                            parts = text.split(':', 1)
                            if len(parts) == 2:
                                specs[parts[0].strip()] = parts[1].strip()
                except:
                    continue
        
        print(f"üìä Total de especifica√ß√µes encontradas: {len(specs)}")
        return specs


    def _extract_product_details(self, basic_product: Dict) -> Optional[Dict[str, Any]]:
        """Vers√£o melhorada da extra√ß√£o de detalhes"""
        try:
            product_url = basic_product['url']
            
            # Acessar p√°gina individual do produto
            print(f"üåê Acessando p√°gina individual...")
            response = self.session.get(product_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrair informa√ß√µes detalhadas
            detailed_product = basic_product.copy()
            
            # Nome mais preciso - SELETORES MELHORADOS
            name_selectors = [
                'h1.page-title span',      # Magento comum
                'h1.page-title',
                '.product-info-main h1',
                '.product-name h1',
                'h1.product-title',
                'h1',
                '.main-product-name'
            ]
            detailed_name = self._extract_text_by_selectors(soup, name_selectors)
            if detailed_name:
                detailed_product['name'] = detailed_name
                print(f"üìù Nome atualizado: {detailed_name[:50]}...")
            
            # Pre√ßo atual - SELETORES MELHORADOS
            price_selectors = [
                '.product-info-price .price-wrapper .price',
                '.product-info-main .price .price',
                '.price-box .regular-price .price',
                '.price-box .special-price .price',
                '.product-price .price',
                '[data-price-type="finalPrice"] .price',
                '.current-price'
            ]
            price_text = self._extract_text_by_selectors(soup, price_selectors)
            if price_text:
                detailed_product['price'] = self._parse_guarani_price(price_text)
                print(f"üí∞ Pre√ßo: {price_text}")
            
            # Pre√ßo original
            original_price_selectors = [
                '.price-box .old-price .price',
                '.price-box .regular-price .price',
                '[data-price-type="oldPrice"] .price',
                '.was-price'
            ]
            original_price_text = self._extract_text_by_selectors(soup, original_price_selectors)
            if original_price_text:
                detailed_product['original_price'] = self._parse_guarani_price(original_price_text)
            
            # DESCRI√á√ÉO DETALHADA - SELETORES MELHORADOS
            description_selectors = [
                '#product-description-content',
                '.product-info-detailed .product.attribute.description .value',
                '.product.attribute.description .value',
                '.product-description .value',
                '.description .std',
                '.product-collateral .std',
                '.product-tabs .description',
                '.tab-content .description'
            ]
            
            description_parts = []
            for selector in description_selectors:
                try:
                    desc_elem = soup.select_one(selector)
                    if desc_elem:
                        text = desc_elem.get_text(separator='\n', strip=True)
                        if text and len(text) > 20:
                            description_parts.append(text)
                except:
                    continue
            
            detailed_product['description'] = '\n\n'.join(description_parts[:3])  # M√°ximo 3 se√ß√µes
            print(f"üìù Descri√ß√£o: {len(detailed_product['description'])} caracteres")
            
            # CATEGORIAS - NOVO!
            categories = self._extract_product_categories(soup)
            detailed_product['categories'] = categories
            
            # URLs das imagens - M√âTODO MELHORADO
            image_urls = self._extract_product_image_urls(soup)
            detailed_product['image_urls'] = image_urls
            
            # Especifica√ß√µes t√©cnicas - SELETORES MELHORADOS
            specs = self._extract_specifications_improved(soup)
            if specs:
                detailed_product['specifications'] = specs
            
            # Marca - SELETORES MELHORADOS
            brand_selectors = [
                '.product-info-main .product-brand',
                '.product-brand',
                '[itemprop="brand"]',
                '.manufacturer',
                '.brand-name'
            ]
            brand = self._extract_text_by_selectors(soup, brand_selectors)
            if brand:
                detailed_product['brand'] = brand
            
            # Disponibilidade
            stock_selectors = [
                '.product-info-stock-sku .stock span',
                '.availability',
                '.stock.available span',
                '.in-stock',
                '.product-availability'
            ]
            stock_text = self._extract_text_by_selectors(soup, stock_selectors)
            detailed_product['availability'] = stock_text if stock_text else 'Consultar disponibilidad'
            
            # Metadados
            detailed_product.update({
                'scraped_at': timezone.now().isoformat(),
                'site_id': self.site.id,
                'currency': self.currency,
                'country': 'Paraguay'
            })
            
            return detailed_product
            
        except Exception as e:
            print(f"‚ùå Erro ao extrair detalhes: {str(e)}")
            import traceback
            print(traceback.format_exc())
            return None

    def _is_product_related_image(self, url: str, img_element) -> bool:
        """Verifica se imagem gen√©rica √© relacionada ao produto"""
        url_lower = url.lower()
        
        # Deve ter extens√£o de imagem
        if not any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            return False
        
        # Indicadores positivos
        positive_indicators = [
            'product', 'catalog', 'media', 'gallery',
            'iphone', 'samsung', 'apple', 'lg', 'sony'
        ]
        
        # Indicadores negativos
        negative_indicators = [
            'logo', 'icon', 'sprite', 'badge', 'button',
            'arrow', 'star', 'cart', 'menu', 'banner',
            'footer', 'header', 'thumb', 'small'
        ]
        
        has_positive = any(indicator in url_lower for indicator in positive_indicators)
        has_negative = any(indicator in url_lower for indicator in negative_indicators)
        
        # Verificar dimens√µes do elemento se dispon√≠vel
        width = img_element.get('width')
        height = img_element.get('height')
        
        if width and height:
            try:
                w, h = int(width), int(height)
                if w < 100 or h < 100:  # Muito pequena
                    return False
            except:
                pass
        
        return has_positive and not has_negative

    def _extract_product_image_urls(self, soup: BeautifulSoup) -> List[str]:
        """Extrai todas as URLs de imagens do produto - VERS√ÉO MELHORADA"""
        image_urls = set()
        
        print(f"üîç Procurando imagens na p√°gina...")
        
        # Seletores espec√≠ficos para diferentes estruturas de galeria
        gallery_selectors = [
            # Estruturas modernas de galeria
            '.product-image-gallery img',
            '.gallery-placeholder img',
            '.product-media-gallery img',
            '.fotorama img',
            '.slick-slide img',
            '.swiper-slide img',
            
            # Estruturas de zoom/lightbox
            'img[data-zoom-image]',
            'img[data-full]',
            'img[data-large]',
            '[data-gallery] img',
            
            # Estruturas Magento
            '.product.media img',
            '.product-image-main img',
            '.more-views img',
            
            # Estruturas gen√©ricas
            '.gallery img',
            '.product-gallery img',
            '.images img',
            '.thumbnails img',
            
            # Fallback para imagens com produto no src
            'img[src*="product"]',
            'img[data-src*="product"]',
            'img[src*="catalog"]',
            'img[data-src*="catalog"]'
        ]
        
        for selector in gallery_selectors:
            try:
                images = soup.select(selector)
                print(f"  üéØ Seletor '{selector}': {len(images)} imagens")
                
                for img in images:
                    # Tentar diferentes atributos de fonte de imagem
                    source_attrs = [
                        'data-zoom-image',  # Imagem em alta resolu√ß√£o
                        'data-full',        # Imagem completa
                        'data-large',       # Imagem grande
                        'data-src',         # Lazy loading
                        'src',              # Fonte padr√£o
                        'data-original',    # Original
                        'data-lazy'         # Lazy load
                    ]
                    
                    for attr in source_attrs:
                        img_url = img.get(attr)
                        if img_url:
                            # Limpar e validar URL
                            img_url = img_url.strip()
                            
                            # Resolver URL absoluta
                            if img_url.startswith('//'):
                                full_url = f"https:{img_url}"
                            elif img_url.startswith('/'):
                                full_url = f"{self.base_url}{img_url}"
                            elif img_url.startswith('http'):
                                full_url = img_url
                            else:
                                full_url = urljoin(self.base_url, img_url)
                            
                            # Validar e adicionar
                            if self._is_valid_product_image_url(full_url):
                                image_urls.add(full_url)
                                print(f"    ‚úÖ Imagem encontrada: {full_url}")
                            break
                            
            except Exception as e:
                print(f"  ‚ùå Erro com seletor '{selector}': {str(e)}")
                continue
        
        # Se n√£o encontrou imagens espec√≠ficas, buscar qualquer imagem da p√°gina
        if not image_urls:
            print(f"‚ö†Ô∏è Nenhuma imagem encontrada, tentando busca gen√©rica...")
            all_images = soup.find_all('img')
            
            for img in all_images:
                src = img.get('src') or img.get('data-src')
                if src:
                    full_url = urljoin(self.base_url, src)
                    if self._is_product_related_image(full_url, img):
                        image_urls.add(full_url)
                        print(f"    üì∏ Imagem gen√©rica: {full_url}")
        
        final_urls = list(image_urls)
        
        # ‚úÖ APLICAR LIMITE AQUI TAMB√âM (opcional - para economizar processamento)
        final_urls = final_urls[:self.max_images_per_product]
        
        print(f"üìä Total de imagens selecionadas: {len(final_urls)} (m√°ximo: {self.max_images_per_product})")
        
        return final_urls
    
    def _validate_image_content(self, content: bytes) -> bool:
        """Valida se o conte√∫do √© realmente uma imagem"""

        if not content or len(content) < 10:
            return False
        
        # ‚úÖ Verificar assinaturas de arquivo de imagem
        image_signatures = {
            b'\xff\xd8\xff': 'JPEG',
            b'\x89PNG\r\n\x1a\n': 'PNG', 
            b'GIF87a': 'GIF',
            b'GIF89a': 'GIF',
            b'RIFF': 'WEBP',  # WEBP come√ßa com RIFF
            b'BM': 'BMP'
        }
        
        for signature, format_name in image_signatures.items():
            if content.startswith(signature):
                print(f"    üîç Formato detectado: {format_name}")
                return True
        
        print(f"    ‚ö†Ô∏è Assinatura n√£o reconhecida: {content[:10]}")
        return False  # Se n√£o reconhecer, melhor rejeitar

    def _extract_specifications(self, soup: BeautifulSoup) -> Dict[str, str]:
        """Extrai especifica√ß√µes t√©cnicas do produto"""
        specs = {}
        
        # Seletores para tabelas de especifica√ß√µes
        spec_selectors = [
            '.additional-attributes-wrapper .data-table tr',
            '.product-specifications tr',
            '.product-attributes tr',
            '.data-table tr'
        ]
        
        for selector in spec_selectors:
            try:
                rows = soup.select(selector)
                for row in rows:
                    cells = row.select('td, th')
                    if len(cells) >= 2:
                        key = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        if key and value:
                            specs[key] = value
            except:
                continue
        
        return specs
    
    def _download_product_images(self, product_data: Dict) -> int:
        """Baixa todas as imagens do produto - VERS√ÉO CORRIGIDA"""
        image_urls = product_data.get('image_urls', [])
        if not image_urls:
            return 0
        
        downloaded_count = 0
        
        for i, img_url in enumerate(image_urls[:self.max_images_per_product]):
            try:
                print(f"  üì∏ Baixando imagem {i+1}/{min(len(image_urls), self.max_images_per_product)}: {img_url}")
                
                # ‚úÖ CORRE√á√ÉO 1: Headers espec√≠ficos para imagens
                image_headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                    'Accept-Encoding': 'identity',  # ‚úÖ Evitar compress√£o que pode causar problemas
                    'Connection': 'keep-alive'
                }
                
                # ‚úÖ CORRE√á√ÉO 2: Download com stream=True e sem encoding autom√°tico
                response = requests.get(
                    img_url, 
                    timeout=30, 
                    stream=True,
                    headers=image_headers,
                    allow_redirects=True
                )
                response.raise_for_status()
                
                # ‚úÖ CORRE√á√ÉO 3: Verificar content-type
                content_type = response.headers.get('content-type', '').lower()
                if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'png', 'webp']):
                    print(f"    ‚ö†Ô∏è Content-type suspeito: {content_type}")
                    # Continuar mesmo assim, pode ser imagem v√°lida
                
                # ‚úÖ CORRE√á√ÉO 4: Verificar tamanho antes de baixar tudo
                content_length = response.headers.get('content-length')
                if content_length:
                    size = int(content_length)
                    if size > self.max_image_size:
                        print(f"    ‚ö†Ô∏è Imagem muito grande: {size:,} bytes")
                        continue
                    if size < self.min_image_size:
                        print(f"    ‚ö†Ô∏è Imagem muito pequena: {size:,} bytes")
                        continue
                
                # ‚úÖ CORRE√á√ÉO 5: Ler conte√∫do como bytes, NUNCA como texto
                try:
                    image_content = b''  # Inicializar como bytes
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:  # Filtrar chunks vazios
                            image_content += chunk
                    
                    print(f"    üì• Baixados {len(image_content):,} bytes")
                    
                except Exception as e:
                    print(f"    ‚ùå Erro ao ler conte√∫do: {str(e)}")
                    continue
                
                # ‚úÖ CORRE√á√ÉO 6: Validar que √© realmente uma imagem
                if not self._validate_image_content(image_content):
                    print(f"    ‚ùå Conte√∫do n√£o √© uma imagem v√°lida")
                    continue
                
                # ‚úÖ CORRE√á√ÉO 7: Processar imagem com tratamento de erro robusto
                processed_image = self._process_product_image_safe(image_content, img_url)
                if not processed_image:
                    print(f"    ‚ùå Falha ao processar imagem")
                    continue
                
                # Gerar nome √∫nico
                filename = self._generate_image_filename(product_data, i)
                
                # Salvar temporariamente na mem√≥ria
                if 'processed_images' not in product_data:
                    product_data['processed_images'] = []
                
                product_data['processed_images'].append({
                    'content_base64': base64.b64encode(processed_image['content']).decode('utf-8'),
                    'filename': filename,
                    'original_url': img_url,
                    'width': processed_image['width'],
                    'height': processed_image['height'],
                    'is_main': i == 0,
                    'content_type': processed_image.get('content_type', 'image/jpeg')
                })
                
                downloaded_count += 1
                print(f"    ‚úÖ Imagem processada ({processed_image['width']}x{processed_image['height']})")
                
                # Rate limiting
                time.sleep(1)
                # import json
                # print(json.dumps(processed_image['content'], default=str))  

                
            except requests.RequestException as e:
                print(f"    ‚ùå Erro de rede ao baixar imagem: {str(e)}")
                continue
            except UnicodeDecodeError as e:
                print(f"    ‚ùå Erro de encoding: {str(e)}")
                print(f"    üîß Tentando m√©todo alternativo...")
                # Tentar m√©todo alternativo
                try:
                    alt_content = self._download_image_alternative(img_url)
                    if alt_content:
                        processed_image = self._process_product_image_safe(alt_content, img_url)
                        if processed_image:
                            # Adicionar imagem processada...
                            downloaded_count += 1
                except:
                    pass
                continue
            except Exception as e:
                print(f"    ‚ùå Erro geral ao baixar imagem: {str(e)}")
                continue
        
        return downloaded_count
    
    def _download_image_alternative(self, url: str) -> Optional[bytes]:
        """M√©todo alternativo para download de imagens problem√°ticas"""
        try:
            print(f"    üîÑ Tentando m√©todo alternativo...")
            
            # Usar urllib ao inv√©s de requests
            import urllib.request
            
            # Criar request com headers
            req = urllib.request.Request(
                url,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            )
            
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read()
                
                if len(content) > self.min_image_size:
                    print(f"    ‚úÖ M√©todo alternativo funcionou: {len(content):,} bytes")
                    return content
            
            return None
            
        except Exception as e:
            print(f"    ‚ùå M√©todo alternativo falhou: {str(e)}")
            return None

    def _process_product_image_safe(self, image_content: bytes, original_url: str) -> Optional[Dict[str, Any]]:
        """Processa imagem com tratamento de erro robusto"""
        try:
            # ‚úÖ CORRE√á√ÉO: Garantir que √© bytes
            if not isinstance(image_content, bytes):
                print(f"    ‚ùå Conte√∫do n√£o √© bytes: {type(image_content)}")
                return None
            
            print(f"    üîÑ Processando {len(image_content):,} bytes...")
            
            # ‚úÖ Criar BytesIO de forma segura
            image_buffer = BytesIO(image_content)
            
            # ‚úÖ Abrir imagem com PIL de forma segura
            try:
                img = Image.open(image_buffer)
                img.verify()  # Verificar se √© imagem v√°lida
                
                # Reabrir ap√≥s verify (verify fecha o arquivo)
                image_buffer.seek(0)
                img = Image.open(image_buffer)
                
            except Exception as e:
                print(f"    ‚ùå PIL n√£o conseguiu abrir: {str(e)}")
                return None
            
            # Verificar dimens√µes m√≠nimas
            width, height = img.size
            if width < 50 or height < 50:
                print(f"    ‚ùå Imagem muito pequena: {width}x{height}")
                return None
            
            print(f"    üìê Dimens√µes originais: {width}x{height}")
            
            # ‚úÖ Converter para RGB se necess√°rio (tratamento robusto)
            try:
                if img.mode not in ['RGB', 'L']:  # L = grayscale
                    if img.mode == 'RGBA':
                        # Criar fundo branco para transpar√™ncia
                        rgb_img = Image.new('RGB', img.size, (255, 255, 255))
                        rgb_img.paste(img, mask=img.split()[-1])
                        img = rgb_img
                    elif img.mode == 'P':
                        # Paleta de cores
                        img = img.convert('RGBA')
                        rgb_img = Image.new('RGB', img.size, (255, 255, 255))
                        rgb_img.paste(img, mask=img.split()[-1] if len(img.split()) > 3 else None)
                        img = rgb_img
                    else:
                        # Outros modos
                        img = img.convert('RGB')
                        
            except Exception as e:
                print(f"    ‚ö†Ô∏è Erro na convers√£o, usando original: {str(e)}")
            
            # Redimensionar se muito grande
            max_dimension = 1200
            if width > max_dimension or height > max_dimension:
                print(f"    üîÑ Redimensionando de {width}x{height}")
                img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
                width, height = img.size
                print(f"    üìê Novas dimens√µes: {width}x{height}")
            
            # ‚úÖ Salvar de forma segura
            output_buffer = BytesIO()
            
            try:
                # Salvar como JPEG com qualidade boa
                img.save(
                    output_buffer, 
                    format='JPEG', 
                    quality=85, 
                    optimize=True,
                    progressive=True  # JPEG progressivo
                )
                
                processed_content = output_buffer.getvalue()
                
                if not processed_content:
                    print(f"    ‚ùå Conte√∫do processado vazio")
                    return None
                
                print(f"    ‚úÖ Processada: {len(processed_content):,} bytes")
                
                return {
                    'content': processed_content,
                    'width': width,
                    'height': height,
                    'format': 'JPEG',
                    'content_type': 'image/jpeg'
                }
                
            except Exception as e:
                print(f"    ‚ùå Erro ao salvar imagem processada: {str(e)}")
                return None
            
        except Exception as e:
            print(f"    ‚ùå Erro geral no processamento: {str(e)}")
            import traceback
            print(f"    üîç Traceback: {traceback.format_exc()}")
            return None
        
        finally:
            # ‚úÖ Limpar buffers
            try:
                if 'image_buffer' in locals():
                    image_buffer.close()
                if 'output_buffer' in locals():
                    output_buffer.close()
            except:
                pass

    def _is_valid_image_signature(self, content: bytes) -> bool:
        """Verifica assinatura do arquivo para confirmar que √© uma imagem v√°lida"""
        if len(content) < 4:
            return False
        
        # Assinaturas de arquivos de imagem
        signatures = {
            b'\xFF\xD8\xFF': 'JPEG',
            b'\x89\x50\x4E\x47': 'PNG',
            b'\x47\x49\x46\x38': 'GIF',
            b'\x52\x49\x46\x46': 'WEBP',  # RIFF (pode ser WEBP)
            b'\x00\x00\x01\x00': 'ICO',
            b'\x42\x4D': 'BMP'
        }
        
        for signature, format_name in signatures.items():
            if content.startswith(signature):
                print(f"    ‚úÖ Formato detectado: {format_name}")
                return True
        
        print(f"    ‚ö†Ô∏è Assinatura desconhecida: {content[:8].hex()}")
        return False

    def _process_product_image(self, image_content: bytes) -> Optional[Dict[str, Any]]:
        """Processa imagem com PIL"""
        try:
            # Abrir imagem
            img = Image.open(BytesIO(image_content))
            
            # Verificar dimens√µes m√≠nimas
            width, height = img.size
            if width < 100 or height < 100:
                return None
            
            # Converter para RGB se necess√°rio
            if img.mode in ['RGBA', 'P']:
                rgb_img = Image.new('RGB', img.size, (255, 255, 255))
                if img.mode == 'P':
                    img = img.convert('RGBA')
                rgb_img.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                img = rgb_img
            
            # Redimensionar se muito grande
            max_dimension = 1200
            if width > max_dimension or height > max_dimension:
                img.thumbnail((max_dimension, max_dimension), Image.Resampling.LANCZOS)
                width, height = img.size
            
            # Salvar otimizada
            output = BytesIO()
            img.save(output, format='JPEG', quality=85, optimize=True)
            
            return {
                'content': output.getvalue(),
                'width': width,
                'height': height,
                'format': 'JPEG'
            }
            
        except Exception as e:
            return None
    
    def _generate_image_filename(self, product_data: Dict, index: int) -> str:
        """Gera nome √∫nico para imagem"""
        from django.utils.text import slugify
        
        name_slug = slugify(product_data['name'])[:30]
        unique_id = str(uuid.uuid4())[:8]
        
        return f"{name_slug}_{index+1}_{unique_id}.jpg"
    
    def _is_valid_product_image_url(self, url: str) -> bool:
        """Valida√ß√£o melhorada de URLs de imagem"""
        if not url or len(url) < 10:
            return False
        
        url_lower = url.lower()
        
        # Deve ter extens√£o de imagem
        if not any(ext in url_lower for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
            return False
        
        # Indicadores positivos
        positive_indicators = [
            'product', 'catalog', 'media', 'gallery', 'image',
            '/m/', '/l/', '/xl/', 'large', 'full'
        ]
        
        # Indicadores negativos (mais espec√≠ficos)
        negative_indicators = [
            'icon', 'logo', 'sprite', 'badge', 'button', 'star',
            'cart', 'arrow', 'loading', 'placeholder',
            'thumb', 'small', '50x50', '100x100', '150x150',
            'watermark', 'overlay'
        ]
        
        has_positive = any(indicator in url_lower for indicator in positive_indicators)
        has_negative = any(indicator in url_lower for indicator in negative_indicators)
        
        # Verificar dimens√µes na URL
        size_patterns = re.findall(r'(\d{2,4})x(\d{2,4})', url_lower)
        if size_patterns:
            width, height = map(int, size_patterns[0])
            if width < 200 or height < 200:  # Muito pequena
                return False
            if width > 2000 or height > 2000:  # Muito grande (pode ser original)
                return True
        
        return (has_positive or not has_negative) and not has_negative
    
    def _extract_text_by_selectors(self, soup, selectors: List[str]) -> str:
        """Extrai texto usando lista de seletores"""
        for selector in selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(strip=True)
                    if text and len(text) > 1:
                        return text
            except:
                continue
        return ''
    
    def _parse_guarani_price(self, price_text: str) -> Optional[Decimal]:
        """Parse de pre√ßo em Guarani"""
        if not price_text:
            return None
        
        try:
            price_clean = re.sub(r'[^\d.,]', '', price_text)
            if not price_clean:
                return None
            
            if '.' in price_clean and ',' not in price_clean:
                price_clean = price_clean.replace('.', '')
            elif ',' in price_clean and '.' in price_clean:
                parts = price_clean.split(',')
                if len(parts) == 2 and len(parts[1]) <= 2:
                    price_clean = parts[0].replace('.', '') + '.' + parts[1]
                else:
                    price_clean = price_clean.replace('.', '').replace(',', '')
            elif ',' in price_clean:
                parts = price_clean.split(',')
                if len(parts) == 2 and len(parts[1]) <= 2:
                    price_clean = parts[0] + '.' + parts[1]
                else:
                    price_clean = price_clean.replace(',', '')
            
            return Decimal(price_clean)
            
        except:
            return None

    def sanitize_for_json(obj):
        """
        Converte recursivamente objetos n√£o-serializ√°veis para tipos JSON-safe.
        - bytes/bytearray/memoryview -> tenta decodificar utf-8, sen√£o base64 string
        - datetime/date/time -> isoformat
        - Decimal -> float
        - UUID -> str
        - Mapping -> dict recursivo
        - list/tuple/set -> lista recursiva
        - Django QuerySet / Model -> tenta model_to_dict ou lista
        - fallback -> str(obj)
        """
        # tipos triviais
        if obj is None:
            return None
        if isinstance(obj, (str, int, float, bool)):
            return obj

        # bytes / bytearray / memoryview
        if isinstance(obj, (bytes, bytearray, memoryview)):
            b = bytes(obj)
            try:
                return b.decode('utf-8')
            except UnicodeDecodeError:
                return base64.b64encode(b).decode('ascii')

        # datetime / date / time
        if isinstance(obj, (datetime.datetime, datetime.date, datetime.time)):
            try:
                return obj.isoformat()
            except Exception:
                return str(obj)

        # Decimal
        if isinstance(obj, decimal.Decimal):
            try:
                return float(obj)
            except Exception:
                return str(obj)

        # UUID
        if isinstance(obj, uuid.UUID):
            return str(obj)

        # Mapping (dict-like)
        if isinstance(obj, Mapping):
            cleaned = {}
            for k, v in obj.items():
                # garantir chave como string
                key = str(k)
                cleaned[key] = sanitize_for_json(v)
            return cleaned

        # listas / tuplas / sets
        if isinstance(obj, (list, tuple, set)):
            return [sanitize_for_json(i) for i in obj]

        # Django QuerySet / Model
        try:
            from django.db.models import Model, QuerySet
            if isinstance(obj, QuerySet):
                return [sanitize_for_json(item) for item in list(obj)]
            if isinstance(obj, Model):
                try:
                    from django.forms.models import model_to_dict
                    return sanitize_for_json(model_to_dict(obj))
                except Exception:
                    # fallback: retornar PK ou str
                    try:
                        return sanitize_for_json(obj.pk)
                    except Exception:
                        return str(obj)
        except Exception:
            # Django n√£o est√° dispon√≠vel ou houve erro -> seguir
            pass

        # fallback geral
        try:
            return str(obj)
        except Exception:
            return repr(obj)


    def find_first_bad(obj, path="root"):
        """
        Tenta localizar o primeiro elemento que n√£o √© json-serializ√°vel (apenas para debug).
        Retorna uma tupla (path, tipo, repr_truncado, exception_str) ou None.
        """
        try:
            json.dumps(obj)
            return None
        except Exception as e:
            # se for dict
            if isinstance(obj, Mapping):
                for k, v in obj.items():
                    bad = find_first_bad(v, f"{path}.{k}")
                    if bad:
                        return bad
            elif isinstance(obj, (list, tuple)):
                for i, v in enumerate(obj):
                    bad = find_first_bad(v, f"{path}[{i}]")
                    if bad:
                        return bad
            else:
                return (path, type(obj).__name__, repr(obj)[:300], str(e))
        return None


    def scrape_products_with_limit(self, query: str, max_results: int = 20, max_detailed: int = 5) -> List[Dict[str, Any]]:
        """
        Scraping com limite separado para acessar detalhes ‚Äî vers√£o com sanitiza√ß√£o recursiva.
        """
        try:
            print(f"üîç SCRAPING COM LIMITE - Busca por '{query}' no Nissei.com")
            print(f"üìã Buscar na listagem: {max_results} produtos")
            print(f"üîé Acessar detalhes de: {max_detailed} produtos")
            print("=" * 60)
            
            # FASE 1: Obter lista b√°sica de produtos
            print("üìã FASE 1: Obtendo lista de produtos...")
            basic_products = self._get_basic_product_list(query, max_results)
            
            if not basic_products:
                print("‚ùå Nenhum produto encontrado na listagem")
                return []
            
            print(f"‚úÖ {len(basic_products)} produtos encontrados na listagem")
            
            # ‚úÖ APLICAR LIMITE PARA DETALHES
            products_for_details = basic_products[:max_detailed]
            
            print(f"üéØ Selecionados {len(products_for_details)} produtos para extra√ß√£o detalhada")
            print("=" * 60)
            
            # FASE 2: Processar apenas os produtos limitados
            print("üîé FASE 2: Processando produtos selecionados individualmente...")
            detailed_products = []
            
            for i, basic_product in enumerate(products_for_details, 1):
                try:
                    print(f"\nüì± PRODUTO {i}/{len(products_for_details)}: {basic_product.get('name','')[:50]}...")
                    print(f"üåê URL: {basic_product.get('url','')}")
                    
                    # Extrair detalhes do produto individual
                    detailed_product = self._extract_product_details(basic_product)
                    
                    if detailed_product:
                        detailed_products.append(detailed_product)
                        print(f"‚úÖ Produto processado com sucesso")
                        
                        # Baixar imagens
                        print(f"üì∏ Baixando at√© {self.max_images_per_product} imagens...")
                        image_count = self._download_product_images(detailed_product)
                        print(f"üì∏ {image_count} imagens baixadas")
                    else:
                        print("‚ùå Falha ao processar produto")
                    
                    # Rate limiting entre produtos
                    if i < len(products_for_details):
                        print(f"‚è±Ô∏è Aguardando {self.delay_between_products}s...")
                        time.sleep(self.delay_between_products)
                        
                except Exception as e:
                    print(f"‚ùå Erro ao processar produto {i}: {str(e)}")
                    continue
            
            # FASE 3: Adicionar produtos restantes SEM detalhes (apenas b√°sicos)
            remaining_products = basic_products[max_detailed:]
            if remaining_products:
                print(f"\nüìã FASE 3: Adicionando {len(remaining_products)} produtos b√°sicos (sem detalhes)...")
                
                for basic_product in remaining_products:
                    basic_product.update({
                        'scraped_at': timezone.now().isoformat(),
                        'site_id': self.site.id,
                        'currency': self.currency,
                        'country': 'Paraguay',
                        'details_extracted': False,
                        'description': 'Detalhes n√£o extra√≠dos - produto da listagem b√°sica',
                        'categories': [],
                        'specifications': {},
                        'image_urls': [],
                        'processed_images': []
                    })
                    detailed_products.append(basic_product)  # agora dentro do loop
            
            # FASE 4: Salvar no banco de dados
            print(f"\nüíæ FASE 4: Salvando {len(detailed_products)} produtos...")
            saved_count = self._save_products_with_details_flag(detailed_products)
            
            print(f"üéâ SCRAPING CONCLU√çDO!")
            print(f"üìä Total encontrados: {len(basic_products)}")
            print(f"üîé Com detalhes extra√≠dos: {len(products_for_details)}")
            print(f"üìã Apenas b√°sicos: {len(remaining_products)}")
            print(f"üíæ Produtos salvos: {saved_count}")
            
            # Montar response_data (igual ao que voc√™ j√° usa)
            response_data = {
                'query': query.strip(),
                'parameters': {
                    'max_results_requested': max_results,
                    'max_detailed_requested': max_detailed,
                    'max_images_per_product': self.max_images_per_product,
                    'actual_detailed_processed': len(detailed_products)
                },
                'site': {
                    'name': getattr(self.site, 'name', None),
                    'url': getattr(self.site, 'url', None),
                    'country': 'Paraguay'
                },
                'scraping_results': {
                    'products_with_details': len(detailed_products),
                    'products': detailed_products
                },
                'database_results': {
                    'saved_products_count': saved_count,
                    # Se voc√™ precisa do serializer aqui, deixe ‚Äî vamos sanitizar tudo depois
                    'products': ProductSerializer(
                        Product.objects.filter(
                            search_query__icontains=query,
                            site=self.site,
                            status__in=[1, 2]
                        ).prefetch_related('images').order_by('-created_at')[:10],
                        many=True,
                        context={'request': None}
                    ).data
                },
                'currency': getattr(self, 'currency', 'Gs.'),
                'timestamp': timezone.now().isoformat(),
                'performance': {
                    'total_time_saved': f"Processou detalhes de apenas {len(detailed_products)} ao inv√©s de {max_results} produtos",
                    'estimated_time': f"~{len(detailed_products) * 8} segundos ao inv√©s de ~{max_results * 8} segundos"
                },
                'success': True
            }

            # FASE 5: Sanitizar recursivamente TODO response_data antes de retornar
            sanitized = sanitize_for_json(response_data)

            # Checagem extra: tentar serializar para JSON e, em caso de erro, imprimir diagn√≥stico
            try:
                json.dumps(sanitized)
            except Exception as e:
                print("‚ùå ERRO ao serializar response_data ap√≥s sanitiza√ß√£o:", str(e))
                bad = find_first_bad(sanitized)
                if bad:
                    print("üîç Primeiro campo problem√°tico encontrado:", bad)
                else:
                    print("üîç N√£o foi poss√≠vel localizar automaticamente o campo problem√°tico.")
            
            return sanitized
            
        except Exception as e:
            print(f"‚ùå Erro geral no scraping com limite: {str(e)}")
            import traceback
            print(traceback.format_exc())
            return []


    def _save_products_with_details_flag(self, products: List[Dict[str, Any]]) -> int:
        """Salva produtos com flag indicando se tem detalhes extra√≠dos"""
        saved_count = 0
        
        for product_data in products:
            try:
                has_details = product_data.get('details_extracted', True)
                
                # Verificar se produto j√° existe
                existing = Product.objects.filter(
                    url=product_data['url'],
                    site=self.site
                ).first()
                
                if existing:
                    # Atualizar apenas se tem detalhes OU se n√£o tinha antes
                    if has_details or not existing.description:
                        existing.name = product_data['name'][:300]
                        existing.price = product_data.get('price')
                        existing.original_price = product_data.get('original_price')
                        if has_details:  # S√≥ atualizar descri√ß√£o se tem detalhes
                            existing.description = product_data.get('description', '')
                        existing.brand = product_data.get('brand', '')[:100] if product_data.get('brand') else None
                        existing.availability = product_data.get('availability', '')[:100]
                        existing.search_query = product_data.get('search_query', '')
                        existing.updated_at = timezone.now()
                        existing.save()
                    
                    product_obj = existing
                else:
                    # Criar novo produto
                    product_obj = Product.objects.create(
                        name=product_data['name'][:300],
                        price=product_data.get('price'),
                        original_price=product_data.get('original_price'),
                        description=product_data.get('description', ''),
                        url=product_data['url'],
                        brand=product_data.get('brand', '')[:100] if product_data.get('brand') else None,
                        availability=product_data.get('availability', '')[:100],
                        site=self.site,
                        search_query=product_data.get('search_query', ''),
                        status=1
                    )
                
                # Salvar imagens apenas se tem detalhes extra√≠dos
                if has_details and 'processed_images' in product_data:
                    self._save_product_images(product_obj, product_data['processed_images'])
                
                saved_count += 1
                status_text = "completo" if has_details else "b√°sico"
                print(f"üíæ Produto salvo ({status_text}): {product_obj.name[:50]}...")
                
            except Exception as e:
                print(f"‚ùå Erro ao salvar produto: {str(e)}")
                continue
        
        return saved_count

    def debug_page_structure(self, soup: BeautifulSoup):
        """M√©todo para debug - analisa estrutura da p√°gina"""
        print(f"\nüîç DEBUG - ESTRUTURA DA P√ÅGINA")
        print("=" * 40)
        
        # Contar elementos importantes
        images = soup.find_all('img')
        print(f"üì∏ Total de imagens na p√°gina: {len(images)}")
        
        # Mostrar classes de imagem
        img_classes = set()
        for img in images[:20]:  # Primeiras 20
            classes = img.get('class', [])
            if classes:
                img_classes.update(classes)
        
        print(f"üè∑Ô∏è Classes de imagem encontradas: {list(img_classes)[:10]}")
        
        # Procurar divs com 'gallery', 'image', 'photo'
        gallery_divs = soup.find_all('div', class_=lambda x: x and any(word in ' '.join(x) for word in ['gallery', 'image', 'photo', 'media']))
        print(f"üñºÔ∏è Divs relacionadas a galeria: {len(gallery_divs)}")
        
        for div in gallery_divs[:5]:
            classes = ' '.join(div.get('class', []))
            print(f"  üìÇ Classe: {classes}")
        
        # Procurar breadcrumbs
        breadcrumb_elements = soup.find_all(['nav', 'div', 'ol', 'ul'], class_=lambda x: x and 'breadcrumb' in ' '.join(x).lower())
        print(f"üóÇÔ∏è Elementos breadcrumb: {len(breadcrumb_elements)}")

    def _save_detailed_products(self, products: List[Dict[str, Any]]) -> int:
        """Salva produtos detalhados no banco com imagens"""
        saved_count = 0
        
        for product_data in products:
            try:
                # Verificar se produto j√° existe
                existing = Product.objects.filter(
                    url=product_data['url'],
                    site=self.site
                ).first()
                
                if existing:
                    # Atualizar produto existente
                    existing.name = product_data['name'][:300]
                    existing.price = product_data.get('price')
                    existing.original_price = product_data.get('original_price')
                    existing.description = product_data.get('description', '')
                    existing.brand = product_data.get('brand', '')[:100] if product_data.get('brand') else None
                    existing.availability = product_data.get('availability', '')[:100]
                    existing.search_query = product_data.get('search_query', '')
                    existing.updated_at = timezone.now()
                    existing.save()
                    
                    product_obj = existing
                else:
                    # Criar novo produto
                    product_obj = Product.objects.create(
                        name=product_data['name'][:300],
                        price=product_data.get('price'),
                        original_price=product_data.get('original_price'),
                        description=product_data.get('description', ''),
                        url=product_data['url'],
                        brand=product_data.get('brand', '')[:100] if product_data.get('brand') else None,
                        availability=product_data.get('availability', '')[:100],
                        site=self.site,
                        search_query=product_data.get('search_query', ''),
                        status=1
                    )
                
                # Salvar imagens processadas
                if 'processed_images' in product_data:
                    self._save_product_images(product_obj, product_data['processed_images'])
                
                saved_count += 1
                print(f"üíæ Produto salvo: {product_obj.name[:50]}...")
                
            except Exception as e:
                print(f"‚ùå Erro ao salvar produto: {str(e)}")
                continue
        
        return saved_count
    
    def _save_product_images(self, product: Product, processed_images: List[Dict]):
        """Salva imagens processadas no banco - VERS√ÉO CORRIGIDA"""
        try:
            # Remover imagens antigas se existirem
            ProductImage.objects.filter(product=product).delete()
            if product.main_image:
                try:
                    if hasattr(product.main_image, 'path') and os.path.exists(product.main_image.path):
                        os.remove(product.main_image.path)
                except:
                    pass
                product.main_image = None
            
            for i, img_data in enumerate(processed_images):
                try:
                    print(f"  üíæ Salvando imagem {i+1}: {img_data['filename']}")
                    
                    # ‚úÖ CORRE√á√ÉO: Criar ContentFile de forma segura
                    content = img_data['content']
                    if not isinstance(content, bytes):
                        print(f"    ‚ùå Conte√∫do n√£o √© bytes: {type(content)}")
                        continue
                    
                    # Criar arquivo Django com conte√∫do bin√°rio
                    image_file = ContentFile(
                        content, 
                        name=img_data['filename']
                    )
                    
                    # ‚úÖ Criar registro ProductImage
                    product_image = ProductImage.objects.create(
                        product=product,
                        image=image_file,
                        is_main=img_data.get('is_main', False),
                        alt_text=f"{product.name} - Imagem {i+1}",
                        order=i,
                        original_url=img_data['original_url']
                    )
                    
                    # Definir primeira imagem como principal
                    if i == 0:
                        product.main_image = product_image.image
                        product.save()
                    
                    print(f"    ‚úÖ Imagem salva: {img_data['filename']}")
                    
                except Exception as e:
                    print(f"    ‚ùå Erro ao salvar imagem {i+1}: {str(e)}")
                    import traceback
                    print(f"    üîç Traceback: {traceback.format_exc()}")
                    continue
                    
        except Exception as e:
            print(f"‚ùå Erro geral ao salvar imagens: {str(e)}")
